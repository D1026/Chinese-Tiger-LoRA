# ğŸ…Chinese-Tiger-LoRA
### <font color=#871F78>record [2023/07/03]</font>
æœ€è¿‘é¢è¯•äº†å®‰è¿œAIçš„å¯¹é½ä¸å®‰å…¨é¡¹ç›®ï¼Œé¢è¯•ä¸­äº¤æµäº†å…³äºaiæ˜¯å¦å·²ç»â€œä¸»åŠ¨æ„è¯†â€ æˆ–è€…â€œç†è§£èƒ½åŠ›â€å’Œâ€œé€»è¾‘èƒ½åŠ›â€ï¼Œæˆ‘çš„è§‚ç‚¹å€¾å‘äºï¼ˆå¯¹äºå½“å‰çš„åŸºäºAutoregressive LMçš„å¤§æ¨¡å‹ï¼‰æ²¡æœ‰ï¼Œ  
ç›¸åçš„ï¼Œæœ‰å¾ˆå¤šè§‚ç‚¹è®¤ä¸ºï¼Œåœ¨è®¸å¤šæ‰€è°“æ¨ç†æ•°æ®é›†ä¸Šï¼Œå¤§æ¨¡å‹èƒ½å¤Ÿå±•ç°å‡ºé€æ­¥åˆ†æçš„æ¨ç†è¿‡ç¨‹ã€‚åŸºäºæ­¤è®¤ä¸º å¤§æ¨¡å‹æœ‰ä¸€å®šçš„ç†è§£èƒ½åŠ›ã€‚
å®é™…ä¸Šï¼ŒåŒºåˆ† æ¨¡å‹åˆ°åº•æ˜¯æœ‰ç°å®ä¸–ç•Œæ¦‚å¿µé—´çš„ç†è§£èƒ½åŠ›ï¼Œè¿˜æ˜¯â€œèƒŒä¹¦â€ï¼ˆrote learning), é‚£ä¹ˆæˆ‘ä»¬éœ€è¦è®¾è®¡ä¸€ä¸ªæ–°çš„ â€œé€»è¾‘è§„åˆ™â€ ï¼ˆä¿è¯è¿™ä¸ªè§„åˆ™è¶³å¤Ÿå¥‡è‘©ã€æ–°é¢–ï¼Œå†å²ä¸Šæ²¡å‡ºç°è¿‡ï¼Œè®­ç»ƒæ•°æ®é‡Œæ²¡è§è¿‡ï¼‰ï¼Œçœ‹æ¨¡å‹æ˜¯å¦èƒ½æŒ‰ç…§è¿™ä¸ªé€»è¾‘ï¼Œå»è¿›è¡Œæ¨ç†ã€‚  

ç»“æœæ˜¯è®©äººæ‚²è§‚çš„ï¼Œå³ä½¿æ˜¯æœ€ç®€å•çš„é€»è¾‘è§„åˆ™ï¼Œæ¨¡å‹çš„å›ç­”ç»“æœä¹Ÿæ˜¯ä¸€å¡Œç³Šæ¶‚ã€‚  
å¦‚ä¸‹ï¼Œæ˜¯æˆ‘åœ¨æµ‹è¯•è‡ªå·±çš„æ¨¡å‹æ—¶ï¼Œæ›¾ç»ä½¿ç”¨çš„ä¸¤ä¸ªè‡ªåˆ›çš„ä¾‹å­ï¼Œå³ä½¿å¼ºå¦‚ chat-gpt/gpt-4 ä¹Ÿä¸èƒ½å›ç­”æ­£ç¡®ï¼Œå¹¶ä¸”å‡ºç°å¹»è§‰åˆ†æè¿‡ç¨‹ã€‚  

![chatGPT_1](https://raw.githubusercontent.com/D1026/Chinese-Tiger-LoRA/main/picture/chatGPT_1.jpeg)

ä¸ªäººè§‚ç‚¹ï¼Œç›®å‰çš„æ¨¡å‹å¾ˆå¯èƒ½å¹¶æ²¡æœ‰äººä»¬æœŸæœ›çš„â€œè‡ªä¸»èƒ½åŠ›â€ã€â€œæ¨ç†èƒ½åŠ›â€ï¼Œâ€œå¹»è§‰â€ç°è±¡æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä½è¯ã€‚  
ç›¸æ¯”äºâ€œ<font color=red>ç†è§£åŠ›</font>â€ï¼Œå®ƒæ›´åƒä¾ç„¶åœç•™åœ¨â€œå½’çº³â€ã€â€œæ€»ç»“â€ã€â€œæ¨¡ä»¿â€ä¸Šï¼Œè¿™å¯èƒ½ä¸æˆ‘ä»¬æœŸæœ›çš„ â€œç†è§£â€ ä¾ç„¶æœ‰è·ç¦»ã€‚  

### <font color=#871F78>record [2023/04/06]</font>
By testing the check-points during the model training, we found that model not always towards to better.

In the first thousands training steps(batch_size=128, 1 epoch ~= 20,000 steps), model's response was going better in terms of
"helpful", "honest" and "harmless". 

As the training continues, there are some question responses oscillated between getting nicer and getting worse. 
In some extreme occasion, model's response even degraded into outputting repetitive words.(This is certainly unacceptable!)

Without any constraint, 
Supervised FineTuning(SFT) on LLM for small instruct dataset shows improvement.
But with more and more samples for pure SFT, model can't be continuously optimized by user expectations.
I guess this is one reason of why chatGPT(Instruct-GPT) only use 13k SFT dataset, meanwhile Reward set and PPO dataset have 30+k.

Besides, chatGPT's Reward model learned by rank-relation instead of a scalar score, this is another important point. 
Learn to rank, not only reducing the annotation costs, 
but more influentially is, ranking is more stable and easy to learn. 
Because it is the essential way of the real world exist, 
while scalar measurement is only a means for humans to describe the world and exchange the knowledge.

I'm now working on find out a new way for LLM SFT constraint. 
by the way, I'm not consider using KL divergence penalty, as chatGPT/instructGPT did during reinforcement learning training.
If you have any idea, contact me by {ivan_duan@126.com or open an Issues, I will reply soon}.

result-record:
It appears various constraint can't help for training a better SFT model. In contrast, designing a more diverse and effective SFT dataset is even more useful.

å°è¯•äº†å„ç§SFT training é˜¶æ®µçº¦æŸï¼Œæ•ˆæœä¸Šçœ‹èµ·æ¥ï¼Œå¢åŠ â€œçº¦æŸâ€æ¡ä»¶ å¹¶ä¸å¦‚ è®¾è®¡æ›´å¥½ã€æ›´å¤šæ ·æ€§çš„ SFT æ•°æ®é›†ï¼›

### project Information
1. This code based on [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora)
2. We collected all chinese instruction dataset before 2023/04/01 from belows:

    https://github.com/LC1332/Chinese-alpaca-lora/blob/main/data/trans_chinese_alpaca_data.json
    
    https://github.com/hikariming/alpaca_chinese_dataset
    
    https://github.com/LianjiaTech/BELLE
    
    totally about 2.5 M samples. We trained [Bloomz](https://huggingface.co/bigscience/bloomz-3b) model on these data.
    